/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package org.apache.spark.sql.execution.exchange

import org.apache.spark.SparkContext
import org.apache.spark.sql.catalyst.expressions._
import org.apache.spark.sql.catalyst.plans.physical._
import org.apache.spark.sql.catalyst.rules.Rule
import org.apache.spark.sql.execution._
import org.apache.spark.sql.internal.SQLConf
import org.apache.spark.util.logging
import org.apache.spark.sql.catalyst.plans.logical.LogicalPlan
import org.apache.spark.sql.execution.joins.{BroadcastHashJoinExec, ShuffledHashJoinExec, SortMergeJoinExec, BroadcastNestedLoopJoinExec, CartesianProductExec}

/**
 * Ensures that the [[org.apache.spark.sql.catalyst.plans.physical.Partitioning Partitioning]]
 * of input data meets the
 * [[org.apache.spark.sql.catalyst.plans.physical.Distribution Distribution]] requirements for
 * each operator by inserting [[ShuffleExchange]] Operators where required.  Also ensure that the
 * input partition ordering requirements are met.
 */
case class EnsureRequirements(conf: SQLConf) extends Rule[SparkPlan] {

  private def defaultNumPreShufflePartitions: Int = conf.numShufflePartitions

  private def targetPostShuffleInputSize: Long = conf.targetPostShuffleInputSize

  private def adaptiveExecutionEnabled: Boolean = conf.adaptiveExecutionEnabled

  private def minNumPostShufflePartitions: Option[Int] = {
    val minNumPostShufflePartitions = conf.minNumPostShufflePartitions
    if (minNumPostShufflePartitions > 0) Some(minNumPostShufflePartitions) else None
  }

  /**
   * Given a required distribution, returns a partitioning that satisfies that distribution.
   */
  private def createPartitioning(
      requiredDistribution: Distribution,
      numPartitions: Int): Partitioning = {

    requiredDistribution match {
      case AllTuples => SinglePartition
      case ClusteredDistribution(clustering) => { HashPartitioning(clustering, numPartitions) }
      case OrderedDistribution(ordering) => RangePartitioning(ordering, numPartitions)
      case dist => sys.error(s"Do not know how to satisfy distribution $dist")
    }
  }

  /**
   * Adds [[ExchangeCoordinator]] to [[ShuffleExchange]]s if adaptive query execution is enabled
   * and partitioning schemes of these [[ShuffleExchange]]s support [[ExchangeCoordinator]].
   */
  private def withExchangeCoordinator(
      children: Seq[SparkPlan],
      requiredChildDistributions: Seq[Distribution]): Seq[SparkPlan] = {

    logDebug(s"Call withExchangeCoordinator!")
    logDebug(s"defaultNumPreShufflePartitions: " + defaultNumPreShufflePartitions)
    logDebug(s"targetPostShuffleInputSize: " + targetPostShuffleInputSize)
    logDebug(s"adaptiveExecutionEnabled: " + adaptiveExecutionEnabled)

    val supportsCoordinator =
      if (children.exists(_.isInstanceOf[ShuffleExchange])) {
        // Right now, ExchangeCoordinator only support HashPartitionings.
        children.forall {
          case e @ ShuffleExchange(hash: HashPartitioning, _, _) => true
          case child =>
            child.outputPartitioning match {
              case hash: HashPartitioning => true
              case collection: PartitioningCollection =>
                collection.partitionings.forall(_.isInstanceOf[HashPartitioning])
              case _ => false
            }
        }
      } else {
        // In this case, although we do not have Exchange operators, we may still need to
        // shuffle data when we have more than one children because data generated by
        // these children may not be partitioned in the same way.
        // Please see the comment in withCoordinator for more details.
        val supportsDistribution =
          requiredChildDistributions.forall(_.isInstanceOf[ClusteredDistribution])
        children.length > 1 && supportsDistribution
      }

    logDebug(s"\tsupportsCoordinator: " + supportsCoordinator)

    val withCoordinator =
      if (adaptiveExecutionEnabled && supportsCoordinator) {
        val coordinator =
          new ExchangeCoordinator(
            children.length,
            targetPostShuffleInputSize,
            minNumPostShufflePartitions)
        children.zip(requiredChildDistributions).map {
          case (e: ShuffleExchange, _) =>
            // This child is an Exchange, we need to add the coordinator.
            e.copy(coordinator = Some(coordinator))
          case (child, distribution) =>
            // If this child is not an Exchange, we need to add an Exchange for now.
            // Ideally, we can try to avoid this Exchange. However, when we reach here,
            // there are at least two children operators (because if there is a single child
            // and we can avoid Exchange, supportsCoordinator will be false and we
            // will not reach here.). Although we can make two children have the same number of
            // post-shuffle partitions. Their numbers of pre-shuffle partitions may be different.
            // For example, let's say we have the following plan
            //         Join
            //         /  \
            //       Agg  Exchange
            //       /      \
            //    Exchange  t2
            //      /
            //     t1
            // In this case, because a post-shuffle partition can include multiple pre-shuffle
            // partitions, a HashPartitioning will not be strictly partitioned by the hashcodes
            // after shuffle. So, even we can use the child Exchange operator of the Join to
            // have a number of post-shuffle partitions that matches the number of partitions of
            // Agg, we cannot say these two children are partitioned in the same way.
            // Here is another case
            //         Join
            //         /  \
            //       Agg1  Agg2
            //       /      \
            //   Exchange1  Exchange2
            //       /       \
            //      t1       t2
            // In this case, two Aggs shuffle data with the same column of the join condition.
            // After we use ExchangeCoordinator, these two Aggs may not be partitioned in the same
            // way. Let's say that Agg1 and Agg2 both have 5 pre-shuffle partitions and 2
            // post-shuffle partitions. It is possible that Agg1 fetches those pre-shuffle
            // partitions by using a partitionStartIndices [0, 3]. However, Agg2 may fetch its
            // pre-shuffle partitions by using another partitionStartIndices [0, 4].
            // So, Agg1 and Agg2 are actually not co-partitioned.
            //
            // It will be great to introduce a new Partitioning to represent the post-shuffle
            // partitions when one post-shuffle partition includes multiple pre-shuffle partitions.
            val targetPartitioning =
              createPartitioning(distribution, defaultNumPreShufflePartitions)
            assert(targetPartitioning.isInstanceOf[HashPartitioning])
            logDebug(s"\twithCoordinator.targetPartitioning --> ShuffleExchange")
            ShuffleExchange(targetPartitioning, child, Some(coordinator))
        }
      } else {
        // If we do not need ExchangeCoordinator, the original children are returned.
        logDebug(s"\tDo not need ExchangeCoordinator")
        children
      }

    withCoordinator
  }

  def printPlanInfo(p: SparkPlan) : Unit = {

    val requiredChildDistribution = p.requiredChildDistribution
    val requiredChildOrdering = p.requiredChildOrdering
    val simpleString = p.simpleString
    val outputOrdering = p.outputOrdering
    val outputPartitioning = p.outputPartitioning
    val treeString = p.treeString
    val nodeName = p.nodeName
    val toJSON = p.toJSON
    val asCode = p.asCode
    val expression = p.expressions
    val inputSet = p.inputSet
    val numberedTreeString = p.numberedTreeString
    val collectLeaves = p.collectLeaves()
    val verboseStringWithSuffix = p.verboseStringWithSuffix
    val children = p.children

    //logInfo(s"[Current OP] nodeName : " + nodeName)
    //logInfo(s"[Current OP] verboseStringWithSuffix : " + verboseStringWithSuffix)
    //logInfo(s"[Current OP] expression : " + expression)
    //logInfo(s"[Current OP] inputSet : " + inputSet)
    //logInfo(s"[Current OP] numberedTreeString : \n" + numberedTreeString)
    // //logInfo(s"[handleGPTPartitionedJoinOP] simpleString : " + simpleString)
    //logInfo(s"[Current OP] outputPartitioning : " + outputPartitioning.toString + ", # partitions: " + outputPartitioning.numPartitions)
    // //logInfo(s"[handleGPTPartitionedJoinOP] treeString : " + treeString)
    // //logInfo(s"[handleGPTPartitionedJoinOP] toJSON : " + toJSON)
    // //logInfo(s"[handleGPTPartitionedJoinOP] asCode : " + asCode)

    //logInfo(s"[Current OP] # requiredChildDistribution : " + requiredChildDistribution.size)
    requiredChildDistribution.foreach{ d =>
      //logInfo(s"requiredChildDistribution: " + d.toString)
    }
    //logInfo(s"\n\n")
    //logInfo(s"[Current OP] # requiredChildOrdering : " + requiredChildOrdering.size)
    requiredChildOrdering.foreach{ d =>
      //logInfo(s"requiredChildOrdering: " + d.toList)
    }
    //logInfo(s"\n\n")
    //logInfo(s"[Current OP] # outputOrdering : " + outputOrdering.size)
    outputOrdering.foreach{ d =>
      //logInfo(s"outputOrdering: " + d.prettyJson)
    }
    //logInfo(s"\n\n")
    //logInfo(s"[Current OP] # collectLeaves: " + collectLeaves.size)
    collectLeaves.foreach{ d =>
      //logInfo(s"collectLeaves: " + d.prettyJson)
    }
    //logInfo(s"\n----------------------------------------------\n")
  }

  def printChildOpInfo(p: SparkPlan, num: Int) : Unit = {

    val requiredChildDistribution = p.requiredChildDistribution
    val requiredChildOrdering = p.requiredChildOrdering
    val simpleString = p.simpleString
    val outputOrdering = p.outputOrdering
    val outputPartitioning = p.outputPartitioning
    val treeString = p.treeString
    val nodeName = p.nodeName
    val toJSON = p.toJSON
    val asCode = p.asCode
    val expression = p.expressions
    val inputSet = p.inputSet
    val numberedTreeString = p.numberedTreeString
    val collectLeaves = p.collectLeaves()
    val verboseStringWithSuffix = p.verboseStringWithSuffix
    val children = p.children

    //logInfo(s"[Child OP-" + num + "] nodeName : " + nodeName)
    //logInfo(s"[Child OP-" + num + "] verboseStringWithSuffix : " + verboseStringWithSuffix)
    //logInfo(s"[Child OP-" + num + "] expression : " + expression)
    //logInfo(s"[Child OP-" + num + "] inputSet : " + inputSet)
    //logInfo(s"[Child OP-" + num + "] numberedTreeString : \n" + numberedTreeString)
    // //logInfo(s"[handleGPTPartitionedJoinOP] simpleString : " + simpleString)
    //logInfo(s"[Child OP-" + num + "] outputPartitioning : " + outputPartitioning.toString + ", # partitions: " + outputPartitioning.numPartitions)
    // //logInfo(s"[handleGPTPartitionedJoinOP] treeString : " + treeString)
    // //logInfo(s"[handleGPTPartitionedJoinOP] toJSON : " + toJSON)
    // //logInfo(s"[handleGPTPartitionedJoinOP] asCode : " + asCode)
    /*logInfo(s"[Child OP-" + num + "] # requiredChildDistribution : " + requiredChildDistribution.size)
    requiredChildDistribution.foreach{ d =>
      logInfo(s"requiredChildDistribution: " + d.toString)
    }
    logInfo(s"\n\n")
    logInfo(s"[Child OP-" + num + "] # requiredChildOrdering : " + requiredChildOrdering.size)
    requiredChildOrdering.foreach{ d =>
      logInfo(s"requiredChildOrdering: " + d.toList)
    }
    logInfo(s"\n\n")
    logInfo(s"[Child OP-" + num + "] # outputOrdering : " + outputOrdering.size)
    outputOrdering.foreach{ d =>
      logInfo(s"outputOrdering: " + d.prettyJson)
    }
    */
  }

  private def ensureDistributionAndOrdering(operator: SparkPlan): SparkPlan = {
    val requiredChildDistributions: Seq[Distribution] = operator.requiredChildDistribution
    val requiredChildOrderings: Seq[Seq[SortOrder]] = operator.requiredChildOrdering
    var children: Seq[SparkPlan] = operator.children
    assert(requiredChildDistributions.length == children.length)
    assert(requiredChildOrderings.length == children.length)

    /*
    *  Added for GPT
    * */

    var isGPTInvolved = false
    var isGPTPairJoin = false
    var isLeftGPTScan = false
    var isRightGPTScan = false
    var isOneSideShuffleInGPT = false
    var leftHashCode = 0
    var rightHashCode = 0
    var isLeftOnlyShuffle = false
    var isRightOnlyShuffle = false

    def printRequiredChildDistribution(p: SparkPlan) : Unit = {
      val requiredChildDistributions = p.requiredChildDistribution
      var childIdx = 0
      requiredChildDistributions.foreach { d =>
        if (d.isInstanceOf[ClusteredDistribution]) {
          logDebug(s"[" + p.verboseStringWithSuffix + "] requiredChildDistributions: ClusteredDistribution --> " + d.asInstanceOf[ClusteredDistribution].toString)
        } else if (d.isInstanceOf[BroadcastDistribution]) {
          logDebug(s"[" + p.verboseStringWithSuffix + "] requiredChildDistributions: BroadcastDistribution --> " + d.asInstanceOf[BroadcastDistribution].toString)
        } else if (d.isInstanceOf[OrderedDistribution]) {
          logDebug(s"[" + p.verboseStringWithSuffix + "] requiredChildDistributions: OrderedDistribution --> " + d.asInstanceOf[OrderedDistribution].toString)
        } else {
          logDebug(s"[" + p.verboseStringWithSuffix + "] requiredChildDistributions: UnspecifiedDistribution")
        }
        childIdx += 1
      }
    }

    def printPartitioning(p: SparkPlan) : Unit = {

      if (p.outputPartitioning != null) {
        logDebug("OP Name: " + p.simpleString + ", PartitionClass: " + p.outputPartitioning.getClass.getSimpleName + ", numPartition: " + p.outputPartitioning.numPartitions)
      } else {
        logDebug("OP Name: " + p.simpleString)
      }
    }

    def printRequiredChildOrderings(p: SparkPlan) : Unit = {
      val requiredChildOrderings = p.requiredChildOrdering.foreach{
        c =>
          logDebug(s"[" + p.verboseStringWithSuffix + "] requiredChildOrderings: " + c.toString())
      }
    }

    // BroadcastHashJoinExec
    // ShuffledHashJoinExec
    // SortMergeJoinExec
    // BroadcastNestedLoopJoinExec
    // CartesianProductExec

    def isJoinOP(p: SparkPlan): Boolean = {

      if (p.isInstanceOf[BroadcastHashJoinExec] ||
        p.isInstanceOf[ShuffledHashJoinExec] ||
        p.isInstanceOf[SortMergeJoinExec] ||
        p.isInstanceOf[BroadcastNestedLoopJoinExec] ||
        p.isInstanceOf[CartesianProductExec] ||
        p.isInstanceOf[BroadcastNestedLoopJoinExec]) {
        true
      } else {
        false
      }
    }

    def findScanOP(p: SparkPlan): SparkPlan = {

      var curChildOP = p

      while (!curChildOP.isInstanceOf[FileSourceScanExec] && !curChildOP.children.isEmpty) {
        curChildOP = curChildOP.children(0)
      }
      curChildOP
    }

    def printOPInfo(p: SparkPlan): Unit = {

      printRequiredChildDistribution(p)
      printPartitioning(p)
      printRequiredChildOrderings(p)
      logDebug(s"[" + p.verboseStringWithSuffix + "] OutputPartitioning: " + p.outputPartitioning)
    }

    def findScanOPWithInfo(p: SparkPlan): SparkPlan = {

      var curChildOP = p
      printOPInfo(curChildOP)
      while (!curChildOP.isInstanceOf[FileSourceScanExec] && !curChildOP.children.isEmpty) {
        curChildOP = curChildOP.children(0)
        printOPInfo(curChildOP)
      }
      curChildOP
    }

    def traverseProjectFilterScanPlan(p: SparkPlan): Unit = {
      var curChildOP = p

      printOPInfo(p)
      while (!curChildOP.isInstanceOf[FileSourceScanExec]) {
        curChildOP = curChildOP.children(0)

        printOPInfo(curChildOP)
      }
    }

    if (children.length == 2 ) {

      /*
      //logInfo(s"Current Operator")
      printOPInfo(operator)
      //logInfo(s"Left Children")
      traverseProjectFilterScanPlan(children(0))
      //logInfo(s"------------------------------------------------------------------------")
      //logInfo(s"Right Children")
      traverseProjectFilterScanPlan(children(1))
      //logInfo(s"------------------------------------------------------------------------")
      */

      def isGPTPartitiondTable(p: FileSourceScanExec): Boolean = {
        val planTree = p.numberedTreeString
        logDebug(s"isGPTPartitiondTable: " + planTree)
        var isGPTPartitionedTable = false

        val token = planTree.split("GPT:")(1).split(",")
        val GPTConf = token(0).trim
        if (GPTConf == "true") {
          isGPTPartitionedTable = true
        }
        isGPTPartitionedTable
      }

      //logInfo(s"\tLeftChildren\n\n" + operator.children(0).numberedTreeString)
      //logInfo(s"\trightChildren\n\n" + operator.children(1).numberedTreeString)
      //printOPInfo(operator);
      val leftScanOP = findScanOPWithInfo(operator.children(0))
      val rightScanOP = findScanOPWithInfo(operator.children(1))

      if(isGPTInvolved && leftScanOP.nodeName == "InMemoryTableScan") {
        val leftTblInfo = operator.children(0).numberedTreeString.split("FileScan")(1).split('[')(0).split(" ")(2)
        val leftTblDB = leftTblInfo.split('.')(0)
        val leftTblName = leftTblInfo.split('.')(1)
        //logInfo(s"\n\nLeftScanOP: $leftTblDB.$leftTblName\n\n$leftScanOP.verboseStringWithSuffix")
      }

      if(isGPTInvolved && rightScanOP.nodeName == "InMemoryTableScan") {
        val rightTblInfo = operator.children(1).numberedTreeString.split("FileScan")(1).split('[')(0).split(" ")(2)
        val rightTblDB = rightTblInfo.split('.')(0)
        val rightTblName = rightTblInfo.split('.')(1)
        //logInfo(s"\n\nRightScanOP: $rightTblDB.$rightTblName\n\n$rightScanOP.verboseStringWithSuffix")

      }

      if(isGPTInvolved && isJoinOP(operator)) {

        /*  Partitioning
        *     - UnknownPartitioning
        *     - RoundRobinPartitioning
        *     - SinglePartition
        *     - HashPartitioning
        *     - RangePartitioning
        *     - PartitioningCollection
        *     - BroadcastPartitioning
        *
        *   Distribution
        *     - UnspecifiedDistribution
        *     - AllTuples
        *     - ClusteredDistribution
        *     - OrderedDistribution
        *     - BroadcastDistribution
        * */

        logDebug("JoinOP: " + operator.verboseStringWithSuffix)
        val tokens = operator.verboseStringWithSuffix.split(" ")
        var joinOpType = ""
        var leftJoinCol = ""
        var rightJoinCol = ""
        var joinSpec = ""
        var buildSpec = "" // only for hash join
        var idx = 0
        tokens.foreach { c =>

          if(idx == 0) {
            joinOpType = c
          } else if (idx == 1) {

            val t = c.substring(1, c.size-2)
            val tt = t.split("#")
            var idx2 = 0
            tt.foreach{ cc =>
              if(idx2 == 0)
                leftJoinCol = cc
              idx2 += 1
            }

          } else if (idx == 2) {

            val t = c.substring(1, c.size-2)
            val tt = t.split("#")
            var idx2 = 0
            tt.foreach{ cc =>
              if(idx2 == 0)
                rightJoinCol = cc
              idx2 += 1
            }

          } else if (idx == 3) {
            joinSpec = c.substring(0, c.size-1)
          }

          if(tokens.size == 5 && idx == 4) {
            buildSpec = c
          }

          idx += 1
        }

        if(leftScanOP.isInstanceOf[FileSourceScanExec] && rightScanOP.isInstanceOf[FileSourceScanExec]) {
          if(isGPTPartitiondTable(leftScanOP.asInstanceOf[FileSourceScanExec]) || isGPTPartitiondTable(rightScanOP.asInstanceOf[FileSourceScanExec]))
            isGPTInvolved = true
        }

        /*
        printDistribution(operator.children(0).requiredChildDistribution)
        printOutputPartitioning(operator.children(0).outputPartitioning)
        logDebug("\tLeftSelectedGPTCols: " + leftScanOP.asInstanceOf[FileSourceScanExec].selectedJoinColForGPT)
        logDebug("\tLeftJoinCol: " + leftJoinCol)
        leftScanOP.asInstanceOf[FileSourceScanExec].getPartitioningColumn().foreach { c =>
          logDebug("\tLeftPartitioningCol: " + c)
        }
        if(leftScanOP.asInstanceOf[FileSourceScanExec].getPartitioningColumn().contains(leftJoinCol))
          logDebug("\tLeft: possible to exploit GPT!")

        printDistribution(operator.children(1).requiredChildDistribution)
        printOutputPartitioning(operator.children(1).outputPartitioning)
        logDebug("\tRightSelectedGPTCols: " + rightScanOP.asInstanceOf[FileSourceScanExec].selectedJoinColForGPT)
        logDebug("\tRightJoinCol: " + rightJoinCol)
        rightScanOP.asInstanceOf[FileSourceScanExec].getPartitioningColumn().foreach { c =>
          logDebug("\tRightPartitioningCol: " + c)
        }
        if(rightScanOP.asInstanceOf[FileSourceScanExec].getPartitioningColumn().contains(rightJoinCol))
          logDebug("\tRight: possible to exploit GPT!")

        if(operator.children(0).outputPartitioning.isInstanceOf[HashPartitioning]) {
          logDebug("LeftChild-JoinOP (HashPartitioning): " + operator.children(0).outputPartitioning.asInstanceOf[HashPartitioning].verboseStringWithSuffix)
        }

        if(operator.children(1).outputPartitioning.isInstanceOf[HashPartitioning]) {
          logDebug("RightChild-JoinOP (HashPartitioning): " + operator.children(1).outputPartitioning.asInstanceOf[HashPartitioning].verboseStringWithSuffix)
        }
        */

        if(leftScanOP.isInstanceOf[FileSourceScanExec]) {
          isLeftGPTScan = leftScanOP.asInstanceOf[FileSourceScanExec].getPartitioningColumn().contains(leftJoinCol)
        } else {
          isLeftGPTScan = false
        }

        if(rightScanOP.isInstanceOf[FileSourceScanExec]) {
          isRightGPTScan = rightScanOP.asInstanceOf[FileSourceScanExec].getPartitioningColumn().contains(rightJoinCol)
        } else {
          isRightGPTScan = false
        }

        leftHashCode = operator.children(0).verboseStringWithSuffix.hashCode
        rightHashCode = operator.children(1).verboseStringWithSuffix.hashCode

        //findScanOPWithInfo(operator.children(0))
        //findScanOPWithInfo(operator.children(1))

      }

      if (isJoinOP(operator)) {
        val joinOpInfo = operator.verboseStringWithSuffix
        val tokens = joinOpInfo.split(",")
        val OtherJoinType = tokens(tokens.length-1).trim

        //logInfo(s"CurrentOP: " + operator.nodeName + ", join type: " + joinType)

        if ((leftScanOP.isInstanceOf[FileSourceScanExec]) && OtherJoinType == "LeftOuter") {
          leftScanOP.asInstanceOf[FileSourceScanExec].setAsOuterJoinTable()
        } else if ((leftScanOP.isInstanceOf[FileSourceScanExec])&&OtherJoinType == "RightOuter") {
          rightScanOP.asInstanceOf[FileSourceScanExec].setAsOuterJoinTable()
        } else if ((leftScanOP.isInstanceOf[FileSourceScanExec]) &&OtherJoinType == "FullOuter") {
          leftScanOP.asInstanceOf[FileSourceScanExec].setAsOuterJoinTable()
          rightScanOP.asInstanceOf[FileSourceScanExec].setAsOuterJoinTable()
        } else if (OtherJoinType == "LeftSemi") {
          // do nothing
        } else if (OtherJoinType == "LeftAnti") {
          // do nothing
        } else if (OtherJoinType == "Cross") {
          // do nothing
        }
      }
    }

    /*
    if(children.size == 1)
      findScanOPWithInfo(children(0))
    else if(children.size == 2) {
      findScanOPWithInfo(children(0))
      findScanOPWithInfo(children(1))
    }
    */

    // Ensure that the operator's children satisfy their output distribution requirements:
    children = children.zip(requiredChildDistributions).map {
      case (child, distribution) if child.outputPartitioning.satisfies(distribution) => {
        logDebug(s"\t[ensureDistributionAndOrdering] child.outputPartitioning.satisfies(distribution)! \n\t child: " + child.outputPartitioning.toString + "\n\t distribution: " + distribution.toString)
        child
      }

      case (child, BroadcastDistribution(mode)) => {

        if(leftHashCode == child.verboseStringWithSuffix.hashCode) {
          logDebug(s"\tLeft: BroadcastDistribution, child: " + child.outputPartitioning.toString + "with # partitions: " + child.outputPartitioning.numPartitions)
        }
        if(rightHashCode == child.verboseStringWithSuffix.hashCode) {
          logDebug(s"\tRight: BroadcastDistribution, child: " + child.outputPartitioning.toString + "with # partitions: " + child.outputPartitioning.numPartitions)
        }
        logDebug(s"\tBroadcastDistribution, child: " + child.outputPartitioning.toString + "with # partitions: " + child.outputPartitioning.numPartitions)

        BroadcastExchangeExec(mode, child)
      }

      case (child, distribution) => {

        var isLeft = false
        if(leftHashCode == child.verboseStringWithSuffix.hashCode)
          isLeft = true
        var isRight = false
        if(rightHashCode == child.verboseStringWithSuffix.hashCode)
          isRight = true

        isGPTPairJoin = isLeftGPTScan && isRightGPTScan

        if(isGPTInvolved && isGPTPairJoin) {
          logDebug(s"GPT Involved Scan & Join Task: Join Between GPT Partitioned Tables")
          child
        }
        else if(isGPTInvolved && isLeft && isLeftGPTScan) {
          logDebug(s"GPT Involved Scan & Join Task: Left GPT Partitioned Table? No Shuffle")
          child
        } else if (isGPTInvolved && isRight && isRightGPTScan) {
          logDebug(s"GPT Involved Scan & Join Task: Right GPT Partitioned Table? No Shuffle")
          child
        } else {
          if(isGPTInvolved && isLeft) {
            logDebug(s"\tLeft requires ShuffleExchange")
            isLeftOnlyShuffle = true
          }

          if(isGPTInvolved && isRight) {
            logDebug(s"\tRight requires ShuffleExchange")
            isRightOnlyShuffle = true
          }

          isOneSideShuffleInGPT = true
          if(isGPTInvolved) {
            logDebug(s"\tGPT requires ShuffleExchange")
            //children.foreach(c=>logDebug(c.prettyJson))
            ShuffleExchange(createPartitioning(distribution, conf.getConfString("GPTBucket").toInt), child)
          } else {
            var maxChildrenNumPartitions = children.map(_.outputPartitioning.numPartitions).max
            logDebug(s"\tNon-GPT requires ShuffleExchange: maxChildrenNumPartitions: " + maxChildrenNumPartitions + ", defaultNumPreShufflePartitions: " + defaultNumPreShufflePartitions)
            //children.foreach(c=>logDebug(c.prettyJson))
            if(maxChildrenNumPartitions == 0)
              ShuffleExchange(createPartitioning(distribution, defaultNumPreShufflePartitions), child)
            else
              ShuffleExchange(createPartitioning(distribution, maxChildrenNumPartitions), child)
          }
        }
      }
    }

    // If the operator has multiple children and specifies child output distributions (e.g. join),
    // then the children's output partitionings must be compatible:
    def requireCompatiblePartitioning(distribution: Distribution): Boolean = distribution match {
      case UnspecifiedDistribution => false
      case BroadcastDistribution(_) => false
      case _ => true
    }

    if (children.length > 1
        && requiredChildDistributions.exists(requireCompatiblePartitioning)
        && !Partitioning.allCompatible(children.map(_.outputPartitioning))) {

      // First check if the existing partitions of the children all match. This means they are
      // partitioned by the same partitioning into the same number of partitions. In that case,
      // don't try to make them match `defaultPartitions`, just use the existing partitioning.
      var maxChildrenNumPartitions = children.map(_.outputPartitioning.numPartitions).max
      if(isGPTInvolved) {
        maxChildrenNumPartitions = conf.getConfString("GPTBucket").toInt
        logDebug(s"\tmaxChildrenNumPartitions: " + maxChildrenNumPartitions)
      }

      var useExistingPartitioning = children.zip(requiredChildDistributions).forall {
        case (child, distribution) =>
          logDebug(s"\tuse ExistingPartitioning: " + child.outputPartitioning.toString + ", numPartition: " + child.outputPartitioning.numPartitions + "," +
            "Distribution: " + distribution.toString + ", maxChildrenNumPartitions: " + maxChildrenNumPartitions)
          child.outputPartitioning.guarantees(
            createPartitioning(distribution, maxChildrenNumPartitions))
      }

      children = if (useExistingPartitioning) {
        // We do not need to shuffle any child's output.
        logDebug(s"\tuse Existing Partitioning!")
        children
      } else {

        if (isGPTInvolved) {
          logDebug(s"GPT Scan & Join Task! but we need to shuffle at least one child's output.")
        }

        // We need to shuffle at least one child's output.
        // Now, we will determine the number of partitions that will be used by created
        // partitioning schemes.
        //logInfo(s"\tneed to shuffle at least one child's output!")
        val numPartitions = {
          // Let's see if we need to shuffle all child's outputs when we use
          // maxChildrenNumPartitions.
          var shufflesAllChildren = children.zip(requiredChildDistributions).forall {
            case (child, distribution) =>
              !child.outputPartitioning.guarantees(
                createPartitioning(distribution, maxChildrenNumPartitions))
          }

          if (isGPTInvolved && isOneSideShuffleInGPT)
            shufflesAllChildren = false

          // If we need to shuffle all children, we use defaultNumPreShufflePartitions as the
          // number of partitions. Otherwise, we use maxChildrenNumPartitions.
          if (shufflesAllChildren) {
            logDebug(s"\tShuffle all children with defaultNumPreShufflePartitions! --> " + defaultNumPreShufflePartitions)
            defaultNumPreShufflePartitions
          } else {
            logDebug(s"\tShuffle some children with maxChildrenNumPartitions! --> " + maxChildrenNumPartitions)
            maxChildrenNumPartitions
          }
        }


        children.zip(requiredChildDistributions).map {
          case (child, distribution) =>
            val targetPartitioning = createPartitioning(distribution, numPartitions)
            if (child.outputPartitioning.guarantees(targetPartitioning)) {

              if(isRightOnlyShuffle || isLeftOnlyShuffle) {
                if(isRightOnlyShuffle) {
                  if (child.verboseStringWithSuffix.hashCode == rightHashCode) {
                    logDebug(s"\tRightChild] child.outputPartitioning.guarantees(targetPartitioning) with # partitions (Left: noShuffle, Right: need Shuffle): " + numPartitions + " with distribution " + distribution.toString)
                  }
                  else {
                      logDebug(s"\tRightChild] child.outputPartitioning.guarantees(targetPartitioning) with # partitions (Left: noShuffle, Right: need Shuffle): " + numPartitions + " with distribution " + distribution.toString)
                    }
                } else {
                  if (child.verboseStringWithSuffix.hashCode == leftHashCode) {
                    logDebug(s"\tLeftChild] child.outputPartitioning.guarantees(targetPartitioning) with # partitions (Left: need Shuffle, Right: noShuffle): " + numPartitions + " with distribution " + distribution.toString)
                } else {
                    logDebug(s"\tLeftChild]child.outputPartitioning.guarantees(targetPartitioning) with # partitions (Left: need Shuffle, Right: noShuffle): " + numPartitions + " with distribution " + distribution.toString)
                  }
                }
              } else {
                logDebug(s"\tchild.outputPartitioning.guarantees(targetPartitioning) with # partitions: " + numPartitions + " with distribution " + distribution.toString)
              }

              child
            }  else if((isRightOnlyShuffle && child.verboseStringWithSuffix.hashCode == leftHashCode) ||
              (isLeftOnlyShuffle && child.verboseStringWithSuffix.hashCode == rightHashCode)) {
              logDebug(s"\tOne side: GPT Partitioned table + Join Column using GPT Partitioning column, Other side: need shuffle!")
              child
            } else {
              child match {
                // If child is an exchange, we replace it with
                // a new one having targetPartitioning.
                case ShuffleExchange(_, c, _) => {

                  if (child.verboseStringWithSuffix.hashCode == leftHashCode) {
                    logDebug(s"\tShuffle exchange (ShuffleExchange(_, c, _)) for Left (isLeftOnly: " + isLeftOnlyShuffle + ", isRightOnly: " + isRightOnlyShuffle + ") : " + targetPartitioning.numPartitions + " partitions")
                  }
                  else if (child.verboseStringWithSuffix.hashCode == rightHashCode) {
                    logDebug(s"\tShuffle exchange (ShuffleExchange(_, c, _)) for Right (isLeftOnly: " + isLeftOnlyShuffle + ", isRightOnly: " + isRightOnlyShuffle + ") : " + targetPartitioning.numPartitions + " partitions")
                  }
                  else {
                    logDebug(s"\tShuffle exchange (ShuffleExchange(_, c, _)): " + targetPartitioning.numPartitions + " partitions")
                  }
                  ShuffleExchange(targetPartitioning, c)
                }
                case _ => {

                  if (child.verboseStringWithSuffix.hashCode == leftHashCode) {
                    logDebug(s"\tShuffle exchange (_) for Left (isLeftOnly: " + isLeftOnlyShuffle + ", isRightOnly: " + isRightOnlyShuffle + ") : " + targetPartitioning.numPartitions + " partitions")
                  }
                  else if (child.verboseStringWithSuffix.hashCode == rightHashCode) {
                    logDebug(s"\tShuffle exchange (_) for Right (isLeftOnly: " + isLeftOnlyShuffle + ", isRightOnly: " + isRightOnlyShuffle + ") : " + targetPartitioning.numPartitions + " partitions")
                  }
                  else {
                    logDebug(s"\tShuffle exchange (_): " + targetPartitioning.numPartitions + " partitions")
                  }
                  ShuffleExchange(targetPartitioning, child)
                }
              }
            }
        }
      }
    }

    // Now, we need to add ExchangeCoordinator if necessary.
    // Actually, it is not a good idea to add ExchangeCoordinators while we are adding Exchanges.
    // However, with the way that we plan the query, we do not have a place where we have a
    // global picture of all shuffle dependencies of a post-shuffle stage. So, we add coordinator
    // at here for now.
    // Once we finish https://issues.apache.org/jira/browse/SPARK-10665,
    // we can first add Exchanges and then add coordinator once we have a DAG of query fragments.
    children = withExchangeCoordinator(children, requiredChildDistributions)
    //logInfo(s"Check ordering is necessary!")
    // Now that we've performed any necessary shuffles, add sorts to guarantee output orderings:
    children = children.zip(requiredChildOrderings).map { case (child, requiredOrdering) =>
      if (requiredOrdering.nonEmpty) {
        // If child.outputOrdering is [a, b] and requiredOrdering is [a], we do not need to sort.
        val orderingMatched = if (requiredOrdering.length > child.outputOrdering.length) {
          false
        } else {
          requiredOrdering.zip(child.outputOrdering).forall {
            case (requiredOrder, childOutputOrder) =>
              childOutputOrder.satisfies(requiredOrder)
          }
        }
        logDebug(s"\tOrdering is required for this children? " + orderingMatched)

        if (!orderingMatched) {
          logDebug(s"\tOrdering unmatched! Adding SortExec for this child!")
          SortExec(requiredOrdering, global = false, child = child)
        } else {
          logDebug(s"\tOrdering matchted!")
          child
        }
      } else {
        logDebug(s"\tNo require ordering for this children")
        child
      }
    }

    operator.withNewChildren(children)
  }

  def apply(plan: SparkPlan): SparkPlan = plan.transformUp {
    case operator @ ShuffleExchange(partitioning, child, _) =>
      child.children match {
        case ShuffleExchange(childPartitioning, baseChild, _)::Nil =>
          if (childPartitioning.guarantees(partitioning)) child else operator
        case _ => operator
      }
    case operator: SparkPlan => ensureDistributionAndOrdering(operator)
  }
}
